{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV4FyTdB/Ycszd6fFxDbOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JEx2VAN/MarketGap-miner-/blob/main/Project_market_miner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies #"
      ],
      "metadata": {
        "id": "roy0Xotj2Gu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  For data handling\n",
        "!pip install pandas numpy\n",
        "\n",
        "# For basic NLP and sentiment\n",
        "!pip install spacy nltk vaderSentiment\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# For topic modeling\n",
        "!pip install bertopic\n",
        "\n",
        "# For advanced NLP models (used by BERTopic)\n",
        "!pip install transformers\n",
        "\n",
        "# For the dashboard and running it in Colab\n",
        "!pip install streamlit pyngrok\n",
        "\n",
        "#  Dependencies for browser automation (kept from original, though unused in this script)\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-driver\n",
        "!pip install selenium"
      ],
      "metadata": {
        "id": "5HIwcei0VWTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Dashboard Libraries\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "-Do_U3AR0vBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "PROJECT_PATH = '/content/drive/MyDrive/MarketGapMiner'\n",
        "# (Replace YOUR_TOKEN_HERE with your actual token from https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "NGROK_AUTHTOKEN = \"35IH5gYLd49YvDhxJDIQqub0vi2_6u79EiRgWLpQkmuZdH6Sx\"\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Create Project Folder ---\n",
        "if not os.path.exists(PROJECT_PATH):\n",
        "    os.makedirs(PROJECT_PATH)\n",
        "    print(f\"Folder '{PROJECT_PATH}' created!\")\n",
        "else:\n",
        "    print(f\"Folder '{PROJECT_PATH}' already exists.\")"
      ],
      "metadata": {
        "id": "sFjUOUpz0vED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_and_save_data(project_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a simulated dataset of product reviews and saves it to a CSV.\n",
        "    \"\"\"\n",
        "    print(\"Simulating dataset...\")\n",
        "    data = {\n",
        "        'product': np.random.choice(['Asana', 'ClickUp', 'Trello'], 500),\n",
        "        'review_text': [\n",
        "            \"The billing is so confusing. I can never find my invoices.\",\n",
        "            \"I love the UI, it's so clean.\",\n",
        "            \"The interface is a total mess, I can't find anything.\",\n",
        "            \"Why is the pricing so high? It's not worth the cost.\",\n",
        "            \"Your customer support never answers my emails.\",\n",
        "            \"This is the best tool ever!\",\n",
        "            \"The mobile app is buggy and constantly crashes.\",\n",
        "            \"I wish it had more integrations with my other tools.\",\n",
        "            \"The integration with Salesforce is broken.\",\n",
        "            \"The free tier is great, but the paid plans are a huge jump.\",\n",
        "            \"I hate the new interface. Why did they change it?\",\n",
        "            \"Customer support was amazing and solved my problem in 5 minutes.\",\n",
        "            \"The billing department charged me twice! Took weeks to fix.\",\n",
        "            \"Missing a key integration for my workflow.\",\n",
        "            \"The mobile app needs a lot of work. It's almost unusable.\",\n",
        "            \"Reporting is powerful, but the setup is a nightmare.\",\n",
        "            \"The UI is beautiful but it's very slow to load.\",\n",
        "            \"I don't understand the pricing model. It's too complex.\",\n",
        "            \"No customer support on weekends. Very frustrating.\",\n",
        "            \"The automation features are a game-changer.\"\n",
        "        ] * 25  # Repeat list 25 times\n",
        "    }\n",
        "\n",
        "    df_raw = pd.DataFrame(data)\n",
        "\n",
        "    # Save raw data\n",
        "    output_path = f\"{project_path}/simulated_reviews.csv\"\n",
        "    df_raw.to_csv(output_path, index=False)\n",
        "    print(f\"Simulated dataset created and saved to {output_path}\")\n",
        "    return df_raw\n",
        "\n",
        "# --- Execute Data Simulation ---\n",
        "df_raw = simulate_and_save_data(PROJECT_PATH)\n",
        "df_raw.head()"
      ],
      "metadata": {
        "id": "b1fZSeDX0vGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NLP Models\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Applies text cleaning pipeline:\n",
        "    1. Lowercase\n",
        "    2. Remove punctuation\n",
        "    3. Remove numbers\n",
        "    4. Lemmatize and remove stopwords\n",
        "    \"\"\"\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove punctuation\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    # 3. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Lemmatize and remove stopwords\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.text not in stop_words and token.lemma_ not in stop_words\n",
        "    ]\n",
        "\n",
        "    # 5. Join tokens back to a string\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "def analyze_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies VADER sentiment analysis to the 'review_text' column.\n",
        "    Uses the 'compound' score.\n",
        "    \"\"\"\n",
        "    df['sentiment_score'] = df['review_text'].apply(\n",
        "        lambda text: sentiment_analyzer.polarity_scores(text)['compound']\n",
        "    )\n",
        "    return df\n",
        "\n",
        "#Test the cleaning function\n",
        "test_sentence = \"The billing is so confusing and terrible! I hate it 100%.\"\n",
        "print(f\"Original: {test_sentence}\")\n",
        "print(f\"Cleaned: {clean_text(test_sentence)}\")\n",
        "\n",
        "#Apply processing to the main DataFrame\n",
        "print(\"\\nApplying sentiment scores and cleaning text...\")\n",
        "df_raw = analyze_sentiment(df_raw)\n",
        "df_raw['cleaned_text'] = df_raw['review_text'].apply(clean_text)\n",
        "print(\"Sentiment scores and cleaned text added.\")\n",
        "df_raw.head()"
      ],
      "metadata": {
        "id": "6JpHzbnj0vK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_topic_modeling(documents: list) -> (BERTopic, list):\n",
        "    \"\"\"\n",
        "    Fits a BERTopic model to a list of documents.\n",
        "    \"\"\"\n",
        "    print(\"Starting BERTopic analysis...\")\n",
        "    topic_model = BERTopic(min_topic_size=3, verbose=True)\n",
        "    topics, probabilities = topic_model.fit_transform(documents)\n",
        "    print(\"BERTopic analysis complete.\")\n",
        "    return topic_model, topics\n",
        "\n",
        "def calculate_gap_scores(df_pain: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the 'Gap_Score' based on frequency, sentiment, and competitor spread.\n",
        "    \"\"\"\n",
        "    print(\"Calculating Gap Scores...\")\n",
        "    # We only want to score the topics we named\n",
        "    df_scored = df_pain[df_pain['topic_name'] != \"Other\"]\n",
        "\n",
        "    # Group by our new topic_name\n",
        "    df_grouped = df_scored.groupby('topic_name')\n",
        "\n",
        "    # Calculate the 3 parts of our formula\n",
        "    df_gap_scores = df_grouped.agg(\n",
        "        Frequency=('topic_name', 'size'),         # 1. Frequency of Complaint\n",
        "        Avg_Sentiment=('sentiment_score', 'mean'), # 2. Average Negative Sentiment\n",
        "        Competitor_Spread=('product', 'nunique')  # 3. Number of Competitors\n",
        "    ).reset_index()\n",
        "\n",
        "    # 2a. Make Avg_Sentiment positive (a high score should be \"more negative\")\n",
        "    df_gap_scores['Avg_Sentiment'] = abs(df_gap_scores['Avg_Sentiment'])\n",
        "\n",
        "    # Calculate the final Gap Score\n",
        "    df_gap_scores['Gap_Score'] = (\n",
        "        df_gap_scores['Frequency'] *\n",
        "        df_gap_scores['Avg_Sentiment'] *\n",
        "        df_gap_scores['Competitor_Spread']\n",
        "    )\n",
        "\n",
        "    # Sort to find the biggest opportunities\n",
        "    df_gap_scores = df_gap_scores.sort_values(by='Gap_Score', ascending=False)\n",
        "\n",
        "    return df_gap_scores\n",
        "\n",
        "# Filter for negative reviews\n",
        "df_pain = df_raw[df_raw['sentiment_score'] < -0.1].copy()\n",
        "print(f\"Original reviews: {len(df_raw)}\")\n",
        "print(f\"Pain point reviews: {len(df_pain)}\")\n",
        "\n",
        "# Run Topic Modeling\n",
        "documents = df_pain['cleaned_text'].tolist()\n",
        "topic_model, topics = run_topic_modeling(documents)\n",
        "df_pain['topic_id'] = topics\n",
        "\n",
        "#  Manual Topic Mapping\n",
        "print(\"--- Topic Model Keywords ---\")\n",
        "print(topic_model.get_topic_info())\n",
        "print(\"----------------------------\")\n",
        "\n",
        "# This manual step is based on the keywords printed above\n",
        "topic_map = {\n",
        "    0: \"Billing & Pricing\", # Based on 'invoice', 'billing'\n",
        "    1: \"Interface & UI\",    # Based on 'mess', 'interface'\n",
        "    2: \"Customer Support\",  # Based on 'high', 'cost', 'pricing'\n",
        "    3: \"Integrations\",      # Based on 'salesforce', 'integration'\n",
        "    4: \"Mobile App\"         # Based on 'change', 'new', 'hate', 'interface' (likely refers to UI changes, mobile app is an assumption here)\n",
        "}\n",
        "df_pain['topic_name'] = df_pain['topic_id'].map(topic_map).fillna(\"Other\")\n",
        "print(\"\\nTopic names mapped.\")\n",
        "\n",
        "# Calculate Scores & Save\n",
        "df_gap_scores = calculate_gap_scores(df_pain)\n",
        "print(\"\\nGap Scores calculated:\")\n",
        "print(df_gap_scores)\n",
        "\n",
        "#Save final data for the dashboard\n",
        "df_gap_scores.to_csv(f\"{PROJECT_PATH}/gap_scores.csv\", index=False)\n",
        "df_pain.to_csv(f\"{PROJECT_PATH}/pain_reviews.csv\", index=False)\n",
        "\n",
        "print(\"\\nFinal data saved to Google Drive.\")\n",
        "df_pain.head()"
      ],
      "metadata": {
        "id": "zYg2041A0vNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# --- Config ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"MarketGap Miner\")\n",
        "\n",
        "# --- File Paths (adjust to your Drive folder) ---\n",
        "PROJECT_PATH = '/content/drive/MyDrive/MarketGapMiner'\n",
        "SCORES_FILE = f'{PROJECT_PATH}/gap_scores.csv'\n",
        "REVIEWS_FILE = f'{PROJECT_PATH}/pain_reviews.csv'\n",
        "\n",
        "#  Load Data\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads the gap scores and review data from the CSV files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_scores = pd.read_csv(SCORES_FILE)\n",
        "        df_reviews = pd.read_csv(REVIEWS_FILE)\n",
        "        return df_scores, df_reviews\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"ERROR: Data files not found. Make sure '{SCORES_FILE}' and '{REVIEWS_FILE}' exist.\")\n",
        "        return None, None\n",
        "\n",
        "# Main App Logic\n",
        "def main():\n",
        "    df_scores, df_reviews = load_data()\n",
        "\n",
        "    if df_scores is None or df_reviews is None:\n",
        "        st.stop()\n",
        "\n",
        "    # Header\n",
        "    st.title(\"ðŸš€ MarketGap Miner\")\n",
        "    st.markdown(\"This dashboard analyzes competitor reviews to find the biggest feature gaps and market opportunities.\")\n",
        "\n",
        "    # 1. Top Opportunities Chart\n",
        "    st.header(\"ðŸ† Top Market Opportunities\")\n",
        "    st.markdown(\"This chart ranks complaint topics by the **Gap Score**. A high score means many people are very angry about this feature across multiple competitors.\")\n",
        "\n",
        "    fig = px.bar(\n",
        "        df_scores,\n",
        "        x='topic_name',\n",
        "        y='Gap_Score',\n",
        "        color='topic_name',\n",
        "        title=\"Ranked Feature Gaps (Higher is a Bigger Opportunity)\",\n",
        "        labels={'topic_name': 'Complaint Topic', 'Gap_Score': 'Gap Score'}\n",
        "    )\n",
        "    fig.update_layout(xaxis_title=None, yaxis_title=\"Gap Score (Higher = More Pain)\")\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    # 2. Drill-Down \"The Evidence\"\n",
        "    st.header(\"ðŸ” Drill-Down: The Evidence\")\n",
        "    st.markdown(\"Select a topic or competitor to read the *actual reviews* that generated the score.\")\n",
        "\n",
        "    # Filters\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        # Get topics from the scores file so it's pre-filtered\n",
        "        topic_list = [\"All Topics\"] + df_scores['topic_name'].unique().tolist()\n",
        "        selected_topic = st.selectbox(\"Filter by Topic:\", topic_list)\n",
        "\n",
        "    with col2:\n",
        "        product_list = [\"All Products\"] + df_reviews['product'].unique().tolist()\n",
        "        selected_product = st.selectbox(\"Filter by Product:\", product_list)\n",
        "\n",
        "    # Filter the \"evidence\" dataframe\n",
        "    df_filtered_reviews = df_reviews.copy()\n",
        "\n",
        "    if selected_topic != \"All Topics\":\n",
        "        df_filtered_reviews = df_filtered_reviews[df_filtered_reviews['topic_name'] == selected_topic]\n",
        "\n",
        "    if selected_product != \"All Products\":\n",
        "        df_filtered_reviews = df_filtered_reviews[df_filtered_reviews['product'] == selected_product]\n",
        "\n",
        "    # Display the filtered reviews\n",
        "    st.dataframe(\n",
        "        df_filtered_reviews[['product', 'review_text', 'sentiment_score', 'topic_name']],\n",
        "        height=400,\n",
        "        use_container_width=True\n",
        "    )\n",
        "    st.success(f\"Showing {len(df_filtered_reviews)} of {len(df_reviews)} total pain point reviews.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QWDUpAlP0vP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit_app():\n",
        "    \"\"\"\n",
        "    Executes the Streamlit app using a shell command.\n",
        "    \"\"\"\n",
        "    os.system('streamlit run app.py')\n",
        "\n",
        "def launch_dashboard(authtoken: str):\n",
        "    \"\"\"\n",
        "    Sets up ngrok, runs the Streamlit app in a thread,\n",
        "    and prints the public URL.\n",
        "    \"\"\"\n",
        "    # Set ngrok auth token\n",
        "    ngrok.set_auth_token(authtoken)\n",
        "\n",
        "    # Run streamlit in a separate thread\n",
        "    print(\"Starting Streamlit app in background thread...\")\n",
        "    thread = threading.Thread(target=run_streamlit_app)\n",
        "    thread.start()\n",
        "\n",
        "    # Wait 5 seconds for the app to start\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Connect ngrok to the streamlit port (8501)\n",
        "    print(\"Connecting to ngrok...\")\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"âœ… Your app is live! Click here: {public_url}\")\n",
        "\n",
        "# --- Launch the app ---\n",
        "launch_dashboard(NGROK_AUTHTOKEN)"
      ],
      "metadata": {
        "id": "go5udsi20vWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIO5dV4m14fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ylcKBz5M0vY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9yth1TO0vbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pG4sVj820veX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CDDILClp0vgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fylq32up0vjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpYUkqAJ0vmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfje8W5R0vrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6_3hwEx0vuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y57mwF2o0vwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1QLyrMM0vz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuE73jH00v21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmiL2zPs0v5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}